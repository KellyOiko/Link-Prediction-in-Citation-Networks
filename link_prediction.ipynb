{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc015b5",
   "metadata": {},
   "source": [
    "\n",
    "Dimitrios Georgitsis 4334\n",
    "\n",
    "Kaliopi Oikonomou 5099\n",
    "\n",
    "\n",
    "\n",
    "Kaggle Names: Dimitris Georgitsis, kaliopiOiko\n",
    "\n",
    "# Link Prediction in Citation Networks\n",
    "\n",
    "Extracting features and building a model to predict citation links between research papers, optimized to minimize log loss.\n",
    "\n",
    "It is a supervised link prediction model that combines NLP (textual similarity) and graph-based features to predict edges in a citation network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c4338",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The code was developed and tested using LightGBM version 3.3.5.\n",
    "Please make sure this version is installed before running the code, either via the terminal or directly inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import log_loss, jaccard_score\n",
    "\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb172ff5",
   "metadata": {},
   "source": [
    "Loading raw data files into pandas DataFrames.\n",
    "\n",
    "\n",
    "Each file contains structured information about papers, including authorship, abstracts, citation edges, and test pairs.\n",
    "\n",
    "\n",
    "The data is parsed and stored in corresponding DataFrames for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bde24ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138499 papers with author information.\n",
      "Loaded 138499 papers with abstracts.\n",
      "Loaded 1091955 citation edges.\n",
      "Loaded 106692 test pairs.\n",
      "   paper_id                                            authors\n",
      "0         0  James H. Niblock,Jian-Xun Peng,Karen R. McMene...\n",
      "1         1              Jian-Xun Peng,Kang Li,De-Shuang Huang\n",
      "2         2                                        J. Heikkila\n",
      "3         3         L. Teslic,B. Hartmann,O. Nelles,I. Skrjanc\n",
      "4         4      Long Zhang,Kang Li,Er-Wei Bai,George W. Irwin\n",
      "   paper_id                                           abstract\n",
      "0         0  The development of an automated system for the...\n",
      "1         1  This paper proposes a novel hybrid forward alg...\n",
      "2         2  Modern CCD cameras are usually capable of a sp...\n",
      "3         3  This paper deals with the problem of fuzzy non...\n",
      "4         4  A number of neural networks can be formulated ...\n",
      "   source  target\n",
      "0       0       1\n",
      "1       0       2\n",
      "2       1       3\n",
      "3       1       5\n",
      "4       1       6\n",
      "   source  target\n",
      "0   34977   59394\n",
      "1   22518   46602\n",
      "2   36762   22813\n",
      "3   44960  110384\n",
      "4   29015   26366\n",
      "testing:  \n",
      "{2}\n"
     ]
    }
   ],
   "source": [
    "# Load authors data\n",
    "authors_data = []\n",
    "with open(\"authors.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        paper_id, _, author_list = line.strip().partition('|--|')\n",
    "        authors_data.append((int(paper_id), author_list))\n",
    "authors_df = pd.DataFrame(authors_data, columns=[\"paper_id\", \"authors\"])\n",
    "\n",
    "# Load abstracts data\n",
    "abstracts_data = []\n",
    "with open(\"abstracts.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        paper_id, _, abstract_text = line.strip().partition('|--|')\n",
    "        abstracts_data.append((int(paper_id), abstract_text))\n",
    "abstracts_df = pd.DataFrame(abstracts_data, columns=[\"paper_id\", \"abstract\"])\n",
    "\n",
    "\n",
    "# Load edges data\n",
    "edges_data = []\n",
    "with open(\"edgelist.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        node1, node2 = map(int, line.strip().split(','))\n",
    "        edges_data.append((node1, node2))\n",
    "edges_df = pd.DataFrame(edges_data, columns=[\"source\", \"target\"])\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "with open(\"test.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        node1, node2 = map(int, line.strip().split(','))\n",
    "        test_data.append((node1, node2))\n",
    "test_df = pd.DataFrame(test_data, columns=[\"source\", \"target\"])\n",
    "\n",
    "\n",
    "# The following data cleaning steps (e.g., removing duplicates and NaNs) were initially considered, \n",
    "# but later deemed unnecessary since the data appeared to be already clean and consistent.\n",
    "\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "#authors_df.drop_duplicates(inplace = True)\n",
    "#authors_df.dropna(inplace = True)\n",
    "\n",
    "#abstracts_df.drop_duplicates(inplace = True)\n",
    "#abstracts_df.dropna(inplace = True)\n",
    "\n",
    "#edges_df.drop_duplicates(inplace = True)\n",
    "#edges_df.dropna(inplace = True)\n",
    "\n",
    "# edgelist:\n",
    "# paper id 0 -----> paper id 1\n",
    "# (node) ---> (node)    [citation]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Loaded {authors_df.shape[0]} papers with author information.\")\n",
    "print(f\"Loaded {abstracts_df.shape[0]} papers with abstracts.\")\n",
    "print(f\"Loaded {edges_df.shape[0]} citation edges.\")\n",
    "print(f\"Loaded {test_df.shape[0]} test pairs.\")\n",
    "\n",
    "print(authors_df.head())\n",
    "print(abstracts_df.head())\n",
    "print(edges_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12016b02",
   "metadata": {},
   "source": [
    "## A. Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ef8c9",
   "metadata": {},
   "source": [
    "### From Abstracts\n",
    "\n",
    "We extract textual similarity features between pairs of papers by processing their abstracts using two common techniques:\n",
    "\n",
    "- TF-IDF Cosine Similarity:\n",
    "\n",
    "Each abstract is converted into a numerical vector using TF-IDF. We then compute the cosine similarity between vectors of two papers. A value closer to 1 indicates higher similarity.\n",
    "\n",
    "\n",
    "- Word2Vec Cosine Similarity:\n",
    "\n",
    "We train a Word2Vec model on all abstracts. Each abstract is then represented by the average of its word vectors. Cosine similarity is computed between two such averaged vectors.\n",
    "\n",
    "------\n",
    "\n",
    "To train a binary classification model, we need both positive and negative examples:\n",
    "\n",
    "- Positive examples are paper pairs that actually have a citation link (i.e., they appear in the citation graph edges_df).\n",
    "\n",
    "- Negative examples are randomly sampled pairs of papers that do not have a citation link between them.\n",
    "    We ensure:\n",
    "\n",
    "    - The two papers are not the same (paper1 ≠ paper2)\n",
    "\n",
    "    - The pair doesn't already exist as a real citation (to avoid false negatives)\n",
    "\n",
    "    - The number of negative samples is equal to the number of positive samples to maintain a balanced dataset\n",
    "        \n",
    "        \n",
    "We defined the positive examples with a label of 1, and the negative examples with a label of 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de684ce2",
   "metadata": {},
   "source": [
    "#### Code for Generating Positive Examples for TF-IDF and  Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47535cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(abstracts_df[\"abstract\"]) # article abstracts -> numerical vectors (using TF-IDF)\n",
    "\n",
    "# Function to compute TF-IDF similarity between two papers\n",
    "def get_tfidf_similarity(paper1_id, paper2_id, tfidf_matrix):\n",
    "    return cosine_similarity(tfidf_matrix[paper1_id], tfidf_matrix[paper2_id])[0][0] # similarity score closer to 1 -> higher similarity between the abstracts.\n",
    "    \n",
    "\n",
    "# Word2Vec Feature Extraction\n",
    "# Tokenize abstracts into words\n",
    "tokenized_abstracts = [abstract.split() for abstract in abstracts_df[\"abstract\"]] # each abstract -> list of tokens (words)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_abstracts, vector_size=300, window=5, min_count=2, workers=2) # Each word is represented by a 300-dimensional vector\n",
    "# We experimented with different Word2Vec hyperparameters such as vector_size, window size, and min_count.\n",
    "# After evaluating performance, we selected: vector_size=300, window=5, min_count=2 as they provided a good trade-off between quality and computational efficiency.\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute the average Word2Vec vector for an abstract\n",
    "def get_abstract_vector(abstract, model):\n",
    "    vectors = [model.wv[word] for word in abstract if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)     # For each abstract, compute the average of its word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Compute Word2Vec vectors for all abstracts\n",
    "abstract_vectors = [get_abstract_vector(abstract, word2vec_model) for abstract in tokenized_abstracts]\n",
    "\n",
    "# Function to compute Word2Vec similarity between two papers\n",
    "def get_word2vec_similarity(paper1_id, paper2_id, abstract_vectors):\n",
    "    return cosine_similarity([abstract_vectors[paper1_id]], [abstract_vectors[paper2_id]])[0][0]\n",
    "\n",
    "\n",
    "textual_features = [] # list to store textual similarity features for connected papers\n",
    "\n",
    "\n",
    "# Iterate through the edgelist and compute textual similarity for each pair\n",
    "for _, row in edges_df.iterrows():   # for every edge 'paperid1, paperid2'\n",
    "    paper1_id = row[\"source\"]\n",
    "    paper2_id = row[\"target\"]\n",
    "    \n",
    "    # Ensure paper IDs are within the range of abstracts\n",
    "    if paper1_id < len(abstracts_df) and paper2_id < len(abstracts_df):\n",
    "        # Compute TF-IDF similarity\n",
    "        tfidf_sim = get_tfidf_similarity(paper1_id, paper2_id, tfidf_matrix)\n",
    "        \n",
    "        # Compute Word2Vec similarity\n",
    "        word2vec_sim = get_word2vec_similarity(paper1_id, paper2_id, abstract_vectors)\n",
    "\n",
    "        \n",
    "        # Store textual similarity features for each positive (citation) pair\n",
    "        # Append to features\n",
    "        textual_features.append({\n",
    "            \"source\": paper1_id,\n",
    "            \"target\": paper2_id,\n",
    "            \"tfidf_similarity\": tfidf_sim,\n",
    "            \"word2vec_similarity\": word2vec_sim,\n",
    "            \"label\": 1  # positive example\n",
    "        })\n",
    "        # format of textual_feautes: \n",
    "        #[  {\"source\": 0, \"target\": 1, \"tfidf_similarity\": 0.45, \"word2vec_similarity\": 0.61, \"label\": 1},\n",
    "        #  {\"source\": 0, \"target\": 2, \"tfidf_similarity\": 0.12, \"word2vec_similarity\": 0.37, \"label\": 1},\n",
    "        #  ...]\n",
    "\n",
    "\n",
    "#textual_features_df = pd.DataFrame(textual_features)\n",
    "#print(textual_features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fd1e3",
   "metadata": {},
   "source": [
    "#### Code for Negative Examples for TF-IDF and  Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59327ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing negative samples: 100%|██████████| 1091955/1091955 [19:43<00:00, 922.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source  target  tfidf_similarity  word2vec_similarity  label\n",
      "0       0       1          0.089054             0.806190      1\n",
      "1       0       2          0.145861             0.906708      1\n",
      "2       1       3          0.125001             0.889072      1\n",
      "3       1       5          0.095378             0.839317      1\n",
      "4       1       6          0.286036             0.922433      1\n"
     ]
    }
   ],
   "source": [
    "# Generate negative examples (non-citation pairs) by randomly sampling unconnected paper pairs\n",
    "# For each sampled pair, compute TF-IDF and Word2Vec similarities and assign label = 0\n",
    "# The number of negative samples matches the number of positive samples to maintain class balance\n",
    "\n",
    "\n",
    "# set of positive edges\n",
    "positive_edges = set((row[\"source\"], row[\"target\"]) for _, row in edges_df.iterrows())\n",
    "num_papers = abstracts_df.shape[0]\n",
    "num_negative_samples = len(edges_df)  # same length with positive samples\n",
    "\n",
    "negative_samples = []\n",
    "\n",
    "while len(negative_samples) < num_negative_samples:\n",
    "    paper1_id = random.randint(0, num_papers - 1)\n",
    "    paper2_id = random.randint(0, num_papers - 1)\n",
    "\n",
    "    # pairs must be: (different papers) AND (edges that don't already exist)\n",
    "    if paper1_id != paper2_id and (paper1_id, paper2_id) not in positive_edges:\n",
    "        negative_samples.append((paper1_id, paper2_id))\n",
    "        positive_edges.add((paper1_id, paper2_id))  # Add to the set to avoid selecting this pair again\n",
    "\n",
    "negative_textual_features = []\n",
    "\n",
    "for paper1_id, paper2_id in tqdm(negative_samples, desc=\"Processing negative samples\"):\n",
    "    tfidf_sim = get_tfidf_similarity(paper1_id, paper2_id, tfidf_matrix)\n",
    "    word2vec_sim = get_word2vec_similarity(paper1_id, paper2_id, abstract_vectors)\n",
    "\n",
    "    negative_textual_features.append({\n",
    "        \"source\": paper1_id,\n",
    "        \"target\": paper2_id,\n",
    "        \"tfidf_similarity\": tfidf_sim,\n",
    "        \"word2vec_similarity\": word2vec_sim,\n",
    "        \"label\": 0  # negative example\n",
    "    })\n",
    "\n",
    "    \n",
    "    \n",
    "# Merging positive and negative feature sets into a single DataFrame\n",
    "# DataFrame of (positive + negative) textual features\n",
    "all_textual_features_df = pd.DataFrame(textual_features + negative_textual_features)\n",
    "\n",
    "print(all_textual_features_df.head())\n",
    "\n",
    "# all_features_df.to_csv(\"all_textual_features_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7646c",
   "metadata": {},
   "source": [
    "### From Authors\n",
    "\n",
    "In this section, we extract features based on the authorship information of the papers. The intuition is that shared or similar authorship patterns may indicate a higher likelihood of citation between two papers. We compute:\n",
    "\n",
    "- Jaccard Similarity:\n",
    "    Measures the overlap between the sets of authors for two papers. A higher value suggests more shared authors, which may imply topical or collaborative proximity.\n",
    "\n",
    "- Common Authors (Count):\n",
    "    Counts how many authors appear in both papers. This raw count serves as a simple but effective signal of author-level connection.\n",
    "\n",
    "These features aim to capture structural and collaborative similarity between papers beyond their textual content.\n",
    "\n",
    "We use the same set of positive and negative paper pairs (source-target pairs) as in the textual features extraction. This ensures consistency when combining features later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f771d",
   "metadata": {},
   "source": [
    "#### Code for Generating Positive Examples for Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d91304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute Jaccard similarity between two sets of authors\n",
    "def compute_author_overlap(authors1, authors2):   # Handling multiple authors per paper as comma-separated lists for overlap calculation\n",
    "    set1 = set(authors1.split(','))\n",
    "    set2 = set(authors2.split(','))\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "author_overlap_features = [] # to store author overlap features\n",
    "\n",
    "# Iterate through the edgelist and compute author overlap for each pair\n",
    "for _, row in edges_df.iterrows():\n",
    "    paper1_id = row[\"source\"]\n",
    "    paper2_id = row[\"target\"]\n",
    "    \n",
    "    # Get authors for both papers\n",
    "    authors1 = authors_df[authors_df[\"paper_id\"] == paper1_id][\"authors\"].values[0]\n",
    "    authors2 = authors_df[authors_df[\"paper_id\"] == paper2_id][\"authors\"].values[0]\n",
    "    \n",
    "    # Compute author overlap\n",
    "    overlap = compute_author_overlap(authors1, authors2)\n",
    "    \n",
    "    # Append to features\n",
    "    author_overlap_features.append({\n",
    "        \"source\": paper1_id,\n",
    "        \"target\": paper2_id,\n",
    "        \"author_overlap\": overlap,\n",
    "        \"label\" : 1 # positive example\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "#author_overlap_features_df = pd.DataFrame(author_overlap_features)\n",
    "\n",
    "#print(author_overlap_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37a50",
   "metadata": {},
   "source": [
    "#### Code for Negative Examples for Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca491957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Author overlap for negatives: 100%|██████████| 1091955/1091955 [11:17<00:00, 1611.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source  target  author_overlap  label\n",
      "0       0       1        0.166667      1\n",
      "1       0       2        0.000000      1\n",
      "2       1       3        0.000000      1\n",
      "3       1       5        0.000000      1\n",
      "4       1       6        0.200000      1\n"
     ]
    }
   ],
   "source": [
    "negative_author_overlap_features = []\n",
    "\n",
    "# Computes the 'author_overlap' feature on the already generated negative_samples\n",
    "for paper1_id, paper2_id in tqdm(negative_samples, desc=\"Author overlap for negatives\"):\n",
    "    try:\n",
    "        authors1 = authors_df[authors_df[\"paper_id\"] == paper1_id][\"authors\"].values[0]\n",
    "        authors2 = authors_df[authors_df[\"paper_id\"] == paper2_id][\"authors\"].values[0]\n",
    "        \n",
    "        # authors_df \n",
    "        #paper_id  authors\n",
    "    # e.g.   3      Alice, Bob, Charlie\n",
    "    # authors1 : 'Alice, Bob, Charlie'\n",
    "        \n",
    "        \n",
    "        overlap = compute_author_overlap(authors1, authors2)\n",
    "\n",
    "        negative_author_overlap_features.append({\n",
    "            \"source\": paper1_id,\n",
    "            \"target\": paper2_id,\n",
    "            \"author_overlap\": overlap,\n",
    "            \"label\": 0  # negative example\n",
    "        })\n",
    "    except IndexError:\n",
    "        # if a paper_id is not in authors_df (.values[0] -> error)\n",
    "        continue\n",
    "\n",
    "# DataFrame with the negative author overlap features\n",
    "#negative_author_df = pd.DataFrame(negative_author_overlap_features)\n",
    "\n",
    "\n",
    "# Merging positive and negative feature sets into a single DataFrame\n",
    "#DataFrame of (positive + negative) author_overlap features\n",
    "all_author_overlap_features_df = pd.DataFrame(author_overlap_features + negative_author_overlap_features)\n",
    "\n",
    "print(all_author_overlap_features_df.head())\n",
    "\n",
    "#all_author_overlap_features_df.to_csv(\"authors_features_labeled.csv\", index=False)\n",
    "\n",
    "#  Combining TF-IDF Cosine Similarity and Author Information for Paper Pair Feature Extraction\n",
    "\n",
    "\n",
    "merged_textual_authors_overlap_df = all_textual_features_df.merge(all_author_overlap_features_df, on=[\"source\", \"target\", \"label\"], how=\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c7564",
   "metadata": {},
   "source": [
    "#### Code for Generating Positive Examples for Common Authors\n",
    "\n",
    "\n",
    "Author names in the dataset vary in format (capitalization, punctuation, use of initials). To handle this, we normalized names by converting to lowercase, removing special characters, and standardizing them to an “initials + last name” format. We mapped all variations to a canonical form and created author sets per paper. This allowed accurate calculation of features like common author counts, improving the quality of author-based similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5f1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing common authors: 100%|██████████| 1091955/1091955 [00:50<00:00, 21596.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to normalize author names\n",
    "def normalize_author_name(name):\n",
    "    name = name.lower().strip()\n",
    "    name = re.sub(r\"[^\\w\\s.]\", \"\", name)  # Keeps only letters, numbers, spaces, and dots\n",
    "    return name\n",
    "\n",
    "# Function to convert to 'initials + last name' format\n",
    "def get_initials_lastname(full_name):\n",
    "    words = full_name.split()\n",
    "    if len(words) > 1:\n",
    "        initials = \".\".join([w[0] for w in words[:-1]]) + \".\"\n",
    "        last_name = words[-1]\n",
    "        return initials + \" \" + last_name\n",
    "    return full_name\n",
    "\n",
    "# dictionary of name variations (variation → canonical)\n",
    "author_variants = defaultdict(set)\n",
    "\n",
    "for authors in authors_df[\"authors\"]:\n",
    "    for name in authors.split(\",\"):\n",
    "        norm_name = normalize_author_name(name)\n",
    "        initials_last = get_initials_lastname(norm_name)\n",
    "        author_variants[norm_name].add(norm_name)\n",
    "        author_variants[norm_name].add(initials_last)\n",
    "\n",
    "# Dictionary: variation -> canonical name\n",
    "canonical_lookup = {}\n",
    "for canonical_name, variations in author_variants.items():\n",
    "    for var in variations:\n",
    "        canonical_lookup[var] = canonical_name\n",
    "\n",
    "# author sets for each paper\n",
    "paper_authors = {}\n",
    "\n",
    "for _, row in authors_df.iterrows():\n",
    "    paper_id = row[\"paper_id\"]\n",
    "    author_list = row[\"authors\"]\n",
    "    canonical_authors = set()\n",
    "    for name in author_list.split(\",\"):\n",
    "        norm_name = normalize_author_name(name)\n",
    "        initials_last = get_initials_lastname(norm_name)\n",
    "        canonical_name = canonical_lookup.get(norm_name, canonical_lookup.get(initials_last, norm_name))\n",
    "        canonical_authors.add(canonical_name)\n",
    "    paper_authors[paper_id] = canonical_authors\n",
    "\n",
    "#Calculate the 'common_authors_count' feature for positive examples\n",
    "common_authors_features = []\n",
    "\n",
    "for _, row in tqdm(edges_df.iterrows(), total=len(edges_df), desc=\"Processing common authors\"):\n",
    "    paper1_id = row[\"source\"]\n",
    "    paper2_id = row[\"target\"]\n",
    "\n",
    "    authors1 = paper_authors.get(paper1_id, set())\n",
    "    authors2 = paper_authors.get(paper2_id, set())\n",
    "\n",
    "    common_count = len(authors1.intersection(authors2))\n",
    "\n",
    "    common_authors_features.append({\n",
    "        \"source\": paper1_id,\n",
    "        \"target\": paper2_id,\n",
    "        \"common_authors_count\": common_count,\n",
    "        \"label\": 1 # positive example\n",
    "    })\n",
    "\n",
    "    \n",
    "#common_authors_features_df = pd.DataFrame(common_authors_features)\n",
    "#print(common_authors_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed50190",
   "metadata": {},
   "source": [
    "#### Code for Generating Negative Examples for Common Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec6523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing common authors (negatives): 100%|██████████| 1091955/1091955 [00:01<00:00, 566980.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature 'common_authors_count' for negative samples\n",
    "\n",
    "negative_common_authors_features = []\n",
    "\n",
    "for paper1_id, paper2_id in tqdm(negative_samples, desc=\"Processing common authors (negatives)\"):\n",
    "    authors1 = paper_authors.get(paper1_id, set())\n",
    "    authors2 = paper_authors.get(paper2_id, set())\n",
    "\n",
    "    common_count = len(authors1.intersection(authors2))\n",
    "\n",
    "    negative_common_authors_features.append({\n",
    "        \"source\": paper1_id,\n",
    "        \"target\": paper2_id,\n",
    "        \"common_authors_count\": common_count,\n",
    "        \"label\": 0 # negative example\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Merging positive and negative feature sets into a single DataFrame\n",
    "#DataFrame of (positive + negative) common_authors features\n",
    "all_common_authors_features_df = pd.DataFrame(common_authors_features + negative_common_authors_features)\n",
    "\n",
    "\n",
    "# Data Frame of merged (textual + author_overlap + common) features, based on keys: (source, target, label)\n",
    "merged_textual_author_overlap_common_df = merged_textual_authors_overlap_df.merge(\n",
    "    all_common_authors_features_df,\n",
    "    on=[\"source\", \"target\", \"label\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "cols = [\n",
    "    \"source\",\n",
    "    \"target\",\n",
    "    \"tfidf_similarity\",\n",
    "    \"word2vec_similarity\",\n",
    "    \"author_overlap\",\n",
    "    \"common_authors_count\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "merged_textual_author_overlap_common_df = merged_textual_author_overlap_common_df[cols]\n",
    "\n",
    "\n",
    "\n",
    "#print(merged_textual_author_overlap_common_df.head())\n",
    "\n",
    "merged_textual_author_overlap_common_df.to_csv(\"features_labeled.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5babd2",
   "metadata": {},
   "source": [
    "### From Graph\n",
    "We create a Directed Graph (DiGraph), meaning edges have direction.\n",
    "edge: paper1 → paper2  (paper1 cites paper2)\n",
    "\n",
    "For each pair (paper1, paper2), we compute the following graph-based features:\n",
    "\n",
    "\n",
    "##### 1. Common Neighbors\n",
    "The number of nodes that are directly connected to both paper1 and paper2.\n",
    "To calculate this, we temporarily treat the graph as undirected.\n",
    "\n",
    "\n",
    "##### 2. Jaccard Coefficient -  Neighborhood Similarity\n",
    "Measures how similar the sets of neighbors of paper1 and paper2 are.\n",
    "\n",
    "If two papers share many of the same neighbors, the Jaccard value will be high.\n",
    "Higher Jaccard ➔ means the papers are more closely related in the graph.\n",
    "    \n",
    "e.g. \n",
    "    -Paper A neighbors: {1, 2, 3, 4}\n",
    "\n",
    "    -Paper B neighbors: {3, 4, 5, 6}\n",
    "\n",
    "    -Common neighbors = {3, 4} ➔ size = 2\n",
    "    \n",
    "    -Union of neighbors = {1,2,3,4,5,6} ➔ size = 6\n",
    "\n",
    "    -Jaccard = 2 / 6 = 0.333\n",
    "\n",
    "\n",
    "##### 3. Adamic-Adar Index\n",
    "\n",
    "Like common neighbors, but gives higher weight to less common neighbors.\n",
    "\n",
    "Rare neighbors (nodes with fewer connections) are considered more informative.\n",
    "A higher Adamic-Adar score → stronger implicit connection between papers.\n",
    "\n",
    "\n",
    "##### 4. Degree \n",
    "In-Degree: Number of papers that cite this paper (higher in-degree = more popular paper).\n",
    "\n",
    "Out-Degree: Number of papers this paper cites (higher out-degree = more active paper).\n",
    "\n",
    "\n",
    "##### 5. Node2Vec Embeddings \n",
    "We generate Node2Vec embeddings by performing biased random walks on the undirected version of the citation graph, learning 16-dimensional vector representations for each paper. These embeddings encode both local neighborhood structure and broader graph connectivity patterns that are not captured by simple graph metrics.\n",
    "\n",
    "For each pair of papers, we compute similarity features using the dot product and cosine similarity of their Node2Vec vectors. These embedding-based similarities are added to the final feature set, improving the model’s ability to capture complex relationships within the citation network.\n",
    "\n",
    "\n",
    "We include two similarity measures between Node2Vec embeddings for each paper pair:\n",
    "\n",
    "- Dot Product (n2v_dot): Measures the raw similarity in vector space, capturing the overall magnitude and alignment of the embeddings. It can be sensitive to vector length.\n",
    "\n",
    "- Cosine Similarity (n2v_cosine): Measures the angle between the two vectors, focusing on their directional similarity regardless of magnitude. This normalizes for vector length differences.\n",
    "\n",
    "\n",
    "-----\n",
    "We also experimented with extracting additional graph-based features:\n",
    "- Preferential Attachment:\n",
    "Measures the product of the degrees of two nodes, reflecting their \"popularity.\" More popular nodes are more likely to connect.\n",
    "\n",
    "- Resource Allocation Index:\n",
    "Estimates how much \"resource\" two nodes share via common neighbors. It is often useful in link prediction tasks.\n",
    "\n",
    "- Shortest Path Length:\n",
    "Measures the minimum number of edges between two nodes in the directed citation graph. It reflects how \"reachable\" one paper is from another.\n",
    "\n",
    "However, these features resulted in worse performance (higher log loss) and were computationally expensive (took too long to compute), so we decided to remove them from the final code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c72727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation graph created \n",
      "Total pairs to process: 2183910\n",
      "Training Node2Vec embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293efa72da874734acbec2cfc0a7495d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [39:23<00:00, 47.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec training complete \n",
      "Calculating Common Neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbors: 100%|██████████| 2183910/2183910 [00:14<00:00, 150214.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Jaccard Coefficient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard Coefficients: 2183910it [00:28, 77307.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Adamic-Adar Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adamic-Adar Scores: 2183910it [00:14, 154039.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Degrees...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building final feature list: 100%|██████████| 2183910/2183910 [08:11<00:00, 4443.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph features DataFrame created \n",
      "Final dataset shape: (2183910, 16)\n",
      "   source  target  tfidf_similarity  word2vec_similarity  author_overlap  \\\n",
      "0   16646   73007          0.407265             0.914058             0.0   \n",
      "1  101985  108800          0.059412             0.846719             0.0   \n",
      "2    3212   17042          0.209458             0.920713             0.0   \n",
      "3   31188   32764          0.000000             0.000000             0.0   \n",
      "4   12509   71431          0.050083             0.855473             0.0   \n",
      "\n",
      "   common_authors_count  common_neighbors  jaccard_coeff  adamic_adar  \\\n",
      "0                     0                 2       0.086957     0.916328   \n",
      "1                     0                 0       0.000000     0.000000   \n",
      "2                     0                 2       0.038462     0.441081   \n",
      "3                     0                 0       0.000000     0.000000   \n",
      "4                     0                 0       0.000000     0.000000   \n",
      "\n",
      "   in_degree_source  out_degree_source  in_degree_target  out_degree_target  \\\n",
      "0                15                  2                 7                  1   \n",
      "1                 5                  1                 3                  0   \n",
      "2                 8                 38                 7                  1   \n",
      "3                 3                  1                12                  3   \n",
      "4                 7                367                 6                  1   \n",
      "\n",
      "    n2v_dot  n2v_cosine  label  \n",
      "0 -0.091231    0.774119      1  \n",
      "1 -0.874845   -1.276049      0  \n",
      "2  0.106630    1.038670      1  \n",
      "3 -0.587783   -0.430123      0  \n",
      "4 -0.661210   -0.211535      1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create the Directed Graph\n",
    "citation_graph = nx.DiGraph()\n",
    "citation_graph.add_edges_from(edges_df.itertuples(index=False, name=None))\n",
    "\n",
    "print(\"Citation graph created \")\n",
    "\n",
    "# Prepare the list of (source, target) pairs\n",
    "edge_pairs = list(zip(merged_textual_author_overlap_common_df[\"source\"], merged_textual_author_overlap_common_df[\"target\"]))\n",
    "\n",
    "print(f\"Total pairs to process: {len(edge_pairs)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Node2Vec Embeddings\n",
    "\n",
    "print(\"Training Node2Vec embeddings...\")\n",
    "\n",
    "# Use an undirected version of the graph (experimentally better for Node2Vec)\n",
    "n2v_graph = citation_graph.to_undirected()\n",
    "\n",
    "# Initialize Node2Vec model\n",
    "#node2vec = Node2Vec(n2v_graph, dimensions=32, walk_length=15, num_walks=30, workers=1, seed=42)  # experimenting with num_walks \n",
    "node2vec = Node2Vec(n2v_graph, dimensions=16, walk_length=20, num_walks=50, workers=1, seed=42)  \n",
    "\n",
    "\n",
    "# Train the Node2Vec model (Word2Vec embedding training)\n",
    "n2v_model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Create embedding dictionary: paper_id -> embedding vector\n",
    "node2vec_embeddings = {int(node): n2v_model.wv.get_vector(str(node)) for node in n2v_graph.nodes}\n",
    "\n",
    "print(\"Node2Vec training complete \")\n",
    "\n",
    "\n",
    "\n",
    "# Compute graph-based features in bulk\n",
    "\n",
    "print(\"Calculating Common Neighbors...\")\n",
    "common_neighbors_dict = {}\n",
    "undirected_graph = citation_graph.to_undirected()\n",
    "for u, v in tqdm(edge_pairs, desc=\"Common Neighbors\"):\n",
    "    try:\n",
    "        common = len(list(nx.common_neighbors(undirected_graph, u, v)))\n",
    "    except:\n",
    "        common = 0\n",
    "    common_neighbors_dict[(u, v)] = common\n",
    "\n",
    "print(\"Calculating Jaccard Coefficient...\")\n",
    "jaccard_scores = nx.jaccard_coefficient(undirected_graph, edge_pairs)\n",
    "jaccard_dict = {(u, v): score for u, v, score in tqdm(jaccard_scores, desc=\"Jaccard Coefficients\")}\n",
    "\n",
    "print(\"Calculating Adamic-Adar Index...\")\n",
    "adamic_scores = nx.adamic_adar_index(undirected_graph, edge_pairs)\n",
    "adamic_dict = {(u, v): score for u, v, score in tqdm(adamic_scores, desc=\"Adamic-Adar Scores\")}\n",
    "\n",
    "print(\"Calculating Degrees...\")\n",
    "in_degrees = dict(citation_graph.in_degree())\n",
    "out_degrees = dict(citation_graph.out_degree())\n",
    "\n",
    "\n",
    "# Combine all features into a final list\n",
    "graph_features = []\n",
    "\n",
    "for u, v in tqdm(edge_pairs, desc=\"Building final feature list\"):\n",
    "\n",
    "\n",
    "        # Embedding-based similarity metrics\n",
    "    if u in node2vec_embeddings and v in node2vec_embeddings:\n",
    "        vec_u = node2vec_embeddings[u]\n",
    "        vec_v = node2vec_embeddings[v]\n",
    "\n",
    "        n2v_dot = np.dot(vec_u, vec_v)\n",
    "        n2v_cosine = cosine_similarity([vec_u], [vec_v])[0][0]\n",
    "    else:\n",
    "        n2v_dot = 0.0\n",
    "        n2v_cosine = 0.0\n",
    "\n",
    "    \n",
    "    graph_features.append({\n",
    "        \"source\": u,\n",
    "        \"target\": v,\n",
    "        \"common_neighbors\": common_neighbors_dict.get((u, v), 0),\n",
    "        \"jaccard_coeff\": jaccard_dict.get((u, v), 0.0),\n",
    "        \"adamic_adar\": adamic_dict.get((u, v), 0.0),\n",
    "        \"in_degree_source\": in_degrees.get(u, 0),\n",
    "        \"out_degree_source\": out_degrees.get(u, 0),\n",
    "        \"in_degree_target\": in_degrees.get(v, 0),\n",
    "        \"out_degree_target\": out_degrees.get(v, 0),\n",
    "        \"n2v_dot\": n2v_dot,          # Node2Vec dot product similarity\n",
    "        \"n2v_cosine\": n2v_cosine,    # Node2Vec cosine similarity\n",
    "\n",
    "    })\n",
    "\n",
    "graph_features_df = pd.DataFrame(graph_features)\n",
    "\n",
    "print(\"Graph features DataFrame created \")\n",
    "\n",
    "# Merge graph features with existing textual + author dataset\n",
    "\n",
    "graph_textual_author_overlap_common_df = merged_textual_author_overlap_common_df.merge(\n",
    "    graph_features_df,\n",
    "    on=[\"source\", \"target\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# Move 'label' column to the end \n",
    "# (for aesthetic and organizational purposes only — does not affect model performance)\n",
    "\n",
    "cols = [col for col in graph_textual_author_overlap_common_df.columns if col != \"label\"] + [\"label\"]\n",
    "graph_textual_author_overlap_common_df = graph_textual_author_overlap_common_df[cols]\n",
    "\n",
    "\n",
    "\n",
    "# Scale the Node2Vec similarity features using the previously fitted scaler\n",
    "# to ensure they have the same distribution as the training data features.\n",
    "# This is important for consistent model performance during inference.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize only the Node2Vec similarity features\n",
    "graph_textual_author_overlap_common_df[[\"n2v_dot\", \"n2v_cosine\"]] = scaler.fit_transform(\n",
    "    graph_textual_author_overlap_common_df[[\"n2v_dot\", \"n2v_cosine\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# Shuffling the dataset before training helps to randomize the order of the samples. \n",
    "# This prevents the model from learning any unintended patterns related to the original data sequence and ensures that training batches are more representative and diverse. \n",
    "# It generally leads to better generalization and more stable training.\n",
    "\n",
    "graph_textual_author_overlap_common_df = shuffle(graph_textual_author_overlap_common_df).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Checking the dataset’s dimensions and sample rows helps verify that all features have been correctly merged and that the data is ready for training or analysis.\n",
    "print(\"Final dataset shape:\", graph_textual_author_overlap_common_df.shape)\n",
    "print(graph_textual_author_overlap_common_df.head())\n",
    "\n",
    "# Optional: Save final dataset to CSV\n",
    "graph_textual_author_overlap_common_df.to_csv(\"final_dataset_with_graph_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15480c",
   "metadata": {},
   "source": [
    "## Β. Training LightGBM Model with Stratified 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca3122",
   "metadata": {},
   "source": [
    "This code trains a LightGBM binary classifier on the final dataset containing graph-based, textual, and author-related features. It uses Stratified K-Fold Cross-Validation (with n=10 folds) to ensure balanced label distribution across splits. For each fold, the model is trained and evaluated using log loss as the performance metric. Early stopping and logging callbacks are used during training for efficiency and monitoring. At the end, the average cross-validation log loss is reported, and trained models are stored for future use or ensemble methods.\n",
    "\n",
    "------\n",
    "\n",
    "We also experimented with the XGBoost model, but it did not yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce78cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0768035\n",
      "[100]\tvalid_0's binary_logloss: 0.0366219\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0366219\n",
      "Fold 1 Log Loss: 0.03662\n",
      "Training fold 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0767561\n",
      "[100]\tvalid_0's binary_logloss: 0.0370133\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0370133\n",
      "Fold 2 Log Loss: 0.03701\n",
      "Training fold 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0760788\n",
      "[100]\tvalid_0's binary_logloss: 0.0359873\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0359873\n",
      "Fold 3 Log Loss: 0.03599\n",
      "Training fold 4/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0762631\n",
      "[100]\tvalid_0's binary_logloss: 0.0361037\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0361037\n",
      "Fold 4 Log Loss: 0.03610\n",
      "Training fold 5/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0748327\n",
      "[100]\tvalid_0's binary_logloss: 0.0345864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0345864\n",
      "Fold 5 Log Loss: 0.03459\n",
      "Training fold 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0768169\n",
      "[100]\tvalid_0's binary_logloss: 0.0370194\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0370194\n",
      "Fold 6 Log Loss: 0.03702\n",
      "Training fold 7/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0761297\n",
      "[100]\tvalid_0's binary_logloss: 0.0359406\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0359406\n",
      "Fold 7 Log Loss: 0.03594\n",
      "Training fold 8/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0760844\n",
      "[100]\tvalid_0's binary_logloss: 0.0360771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0360771\n",
      "Fold 8 Log Loss: 0.03608\n",
      "Training fold 9/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0754101\n",
      "[100]\tvalid_0's binary_logloss: 0.0352512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.0352512\n",
      "Fold 9 Log Loss: 0.03525\n",
      "Training fold 10/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0765832\n",
      "[100]\tvalid_0's binary_logloss: 0.036782\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.036782\n",
      "Fold 10 Log Loss: 0.03678\n",
      "\n",
      "Average CV Log Loss across 10 folds: 0.03614\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset containing features and labels\n",
    "df = pd.read_csv(\"final_dataset_with_graph_features.csv\")\n",
    "\n",
    "# Separate features (X) and target label (y)\n",
    "X = df.drop(columns=[\"label\"]) # input features only - Features for prediction\n",
    "y = df[\"label\"]                # target: 1 = citation, 0 = no citation\n",
    "\n",
    "\n",
    "# Define LightGBM hyperparameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Number of cross-validation folds\n",
    "n_folds = 10\n",
    "\n",
    "# Create Stratified K-Fold object to maintain label balance across folds\n",
    "cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "models = []    # to store trained models\n",
    "cv_scores = [] # to store log loss scores for each fold\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate model on each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"Training fold {fold+1}/{n_folds}...\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    \n",
    "    # Create LightGBM Datasets\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    # Early stopping helps prevent overfitting by monitoring the validation loss.\n",
    "    # If the model's performance does not improve for a specified number of rounds (here: 50), training stops early.\n",
    "    # This ensures that we do not continue training unnecessarily once the model has converged.\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set and compute log loss\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    score = log_loss(y_val_fold, y_val_pred)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"Fold {fold+1} Log Loss: {score:.5f}\")\n",
    "\n",
    "    \n",
    "    # Store the trained model\n",
    "    models.append(model)\n",
    "\n",
    "    \n",
    "\n",
    "# Print average log loss across all folds\n",
    "print(f\"\\nAverage CV Log Loss across {n_folds} folds: {np.mean(cv_scores):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b834123",
   "metadata": {},
   "source": [
    "\n",
    "We chose to use 10 folds for cross-validation (k=10) to balance between reliable performance estimation and computational efficiency. With 10 folds, each training run uses 90% of the data for training and 10% for validation, providing a robust and stable measure of model performance. This setup helps reduce variance in the evaluation metrics and ensures that the model generalizes well. The results from example runs showed consistent log loss scores around 0.138, confirming the stability of this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e945b",
   "metadata": {},
   "source": [
    "Log Loss Formula\n",
    "\n",
    "L=−1/N Σi=1,N(yi*log(pi)+(1−yi)log(1−pi))\n",
    "\n",
    "\n",
    "    N: Number of examples (e.g., number of test pairs)\n",
    "\n",
    "    yᵢ: True label of example i (0 or 1)\n",
    "\n",
    "    pᵢ: Model prediction for example i (probability of class 1, e.g., 0.82)\n",
    "    \n",
    "    \n",
    "\n",
    "If the true label is 1, the model is penalized when pᵢ is close to 0.\n",
    "\n",
    "If the true label is 0, the model is penalized when pᵢ is close to 1.\n",
    "\n",
    "Log Loss = 0: Perfect prediction (e.g., pᵢ = 1 when yᵢ = 1)\n",
    "\n",
    "High Log Loss: When the model is very confident but wrong (e.g., pᵢ = 0.99 while yᵢ = 0)\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "| True Label (y) | Prediction (p) | Log Loss Contribution | Interpretation |\n",
    "|----------------|----------------|------------------------|----------------|\n",
    "| 1              | 0.99           | Low                    | Very good      |\n",
    "| 1              | 0.60           | Moderate               | Okay           |\n",
    "| 1              | 0.10           | High                   | Poor           |\n",
    "| 0              | 0.01           | Low                    | Good           |\n",
    "| 0              | 0.90           | High                   | Bad            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a957b",
   "metadata": {},
   "source": [
    "## C. Feature Extraction From test\n",
    "\n",
    "In this section, we extract the same set of features for the test dataset as we did for the training data. This ensures consistency and allows the trained model to make predictions based on comparable input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd39c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features: 100%|██████████| 106692/106692 [03:41<00:00, 480.92it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load test pairs\n",
    "test_pairs = pd.read_csv(\"test.txt\", names=[\"source\", \"target\"])\n",
    "\n",
    "# Extract features for test pairs\n",
    "test_features = []\n",
    "for _, row in tqdm(test_pairs.iterrows(), total=len(test_pairs), desc=\"Extracting test features\"):\n",
    "    paper1_id, paper2_id = row[\"source\"], row[\"target\"]\n",
    "\n",
    "    # Compute textual similarities\n",
    "    tfidf_sim = get_tfidf_similarity(paper1_id, paper2_id, tfidf_matrix)\n",
    "    word2vec_sim = get_word2vec_similarity(paper1_id, paper2_id, abstract_vectors)\n",
    "\n",
    "    # Compute authorship features \n",
    "    authors1 = authors_df[authors_df[\"paper_id\"] == paper1_id][\"authors\"]\n",
    "    authors2 = authors_df[authors_df[\"paper_id\"] == paper2_id][\"authors\"]\n",
    "\n",
    "    if not authors1.empty and not authors2.empty:\n",
    "        author_overlap = compute_author_overlap(authors1.values[0], authors2.values[0])\n",
    "    else:\n",
    "        author_overlap = 0.0\n",
    "\n",
    "    # Common authors \n",
    "    common_authors = len(paper_authors.get(paper1_id, set()).intersection(paper_authors.get(paper2_id, set())))\n",
    "\n",
    "    # Graph-based features\n",
    "    try:\n",
    "        common_neighbors = len(list(nx.common_neighbors(undirected_graph, paper1_id, paper2_id)))\n",
    "    except:\n",
    "        common_neighbors = 0\n",
    "\n",
    "    try:\n",
    "        jaccard = list(nx.jaccard_coefficient(undirected_graph, [(paper1_id, paper2_id)]))[0][2]\n",
    "    except:\n",
    "        jaccard = 0.0\n",
    "\n",
    "    try:\n",
    "        adamic = list(nx.adamic_adar_index(undirected_graph, [(paper1_id, paper2_id)]))[0][2]\n",
    "    except:\n",
    "        adamic = 0.0\n",
    "        \n",
    "        \n",
    "        # Node2Vec similarity features\n",
    "    if paper1_id in node2vec_embeddings and paper2_id in node2vec_embeddings:\n",
    "        vec_u = node2vec_embeddings[paper1_id]\n",
    "        vec_v = node2vec_embeddings[paper2_id]\n",
    "        n2v_dot = np.dot(vec_u, vec_v)\n",
    "        n2v_cosine = cosine_similarity([vec_u], [vec_v])[0][0]\n",
    "    else:\n",
    "        n2v_dot = 0.0\n",
    "        n2v_cosine = 0.0\n",
    "\n",
    "\n",
    "    test_features.append({\n",
    "        \"source\": paper1_id,\n",
    "        \"target\": paper2_id,\n",
    "        \"tfidf_similarity\": tfidf_sim,\n",
    "        \"word2vec_similarity\": word2vec_sim,\n",
    "        \"author_overlap\": author_overlap,\n",
    "        \"common_authors_count\": common_authors,\n",
    "        \"common_neighbors\": common_neighbors,\n",
    "        \"jaccard_coeff\": jaccard,\n",
    "        \"adamic_adar\": adamic,\n",
    "        \"in_degree_source\": in_degrees.get(paper1_id, 0),\n",
    "        \"out_degree_source\": out_degrees.get(paper1_id, 0),\n",
    "        \"in_degree_target\": in_degrees.get(paper2_id, 0),\n",
    "        \"out_degree_target\": out_degrees.get(paper2_id, 0),\n",
    "        \n",
    "        \"n2v_dot\": n2v_dot,\n",
    "        \"n2v_cosine\": n2v_cosine\n",
    "    })\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features)\n",
    "\n",
    "\n",
    "# Scale the Node2Vec similarity features using the previously fitted scaler\n",
    "test_features_df[[\"n2v_dot\", \"n2v_cosine\"]] = scaler.transform(\n",
    "    test_features_df[[\"n2v_dot\", \"n2v_cosine\"]]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1d82d",
   "metadata": {},
   "source": [
    "Now we use all the trained models to make predictions on the test data. We combine their results by averaging to get more reliable scores. Finally, we save these predictions to a csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad8c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using all models and average the results (ensemble method)\n",
    "test_preds = np.mean([model.predict(test_features_df) for model in models], axis=0)\n",
    "\n",
    "# Save predictions to submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_pairs.index,\n",
    "    \"Label\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856822e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
